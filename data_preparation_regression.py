#!/usr/bin/env python
# coding: utf-8

# Prepare training, validation and test data for regression tasks. Before to excute this program, input and output txt files for MULAN-ACCENT were generated for each audio, as well as egemaps features. Data for MFCC features was generated from fbank_pitch features.


import time
start = time.time()
import os
from glob import glob
import pandas as pd
import numpy as np
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.model_selection import LeavePGroupsOut
from sklearn import mixture
import librosa
import tensorflow as tf
import sys
from scipy.fftpack import dct
import shutil


# # Function definition

# In[2]:

# function generate_scores_utt is to deal with the txt files generated by MULAN-ACCENT: the input feature files fbank_pitch.*.txt and the output speech attribute score file scores.*.txt
def generate_scores_utt(filepath):
    dict_utt_scores = {}
    utt_list = []
    dict_utt_dur = {}
    utt_num = 0
    if feat_type == 'sas':
        txtfiles = glob(os.path.join(filepath, 'scores.*.txt'))
    if feat_type == 'fbank_pitch':
        txtfiles = glob(os.path.join(filepath, 'fbank_pitch.*.txt'))
    for file in txtfiles:
        f = open(file)
        print(os.path.basename(file))
        utt_num_infile = 0
        utt_num_saved = 0
        line = f.readline()
        while line:
            if '[' in line:
                utt_id = line[0:line.index(' ')]
                utt_list.append(utt_id)
                frame_num = 0
        #         print('utterance id: {}'.format(utt_id))
                utt_num = utt_num + 1
                scores = None
                utt_num_infile = utt_num_infile + 1
                
            else:
                frame = np.array(line.strip().strip(' ]').split(' '))
                frame = frame.astype(float)
                frame_num = frame_num + 1
                if scores is None:
                    scores = frame
                else:
                    scores = np.vstack((scores, frame))
                if ']' in line:
#                     print('{}: {} frames'.format(utt_id, frame_num))
                    dict_utt_dur[utt_id] = frame_num
                    dict_utt_scores[utt_id] = scores
                    utt_num_saved = utt_num_saved + 1
            line = f.readline()    
        f.close()
    return dict_utt_dur, dict_utt_scores, utt_list, utt_num


# for one speaker, concatenate the scores matrix of manner and place and compute dynamic features. Each speaker has one excel file.
def concatenate_scores(scores_manner, scores_place, filepath_res, title, dynamic_width):
    if scores_manner.shape[1] == scores_place.shape[1]:
        attributes_place = attributes["place"]
        other_idx = attributes_place.index("other")
        silence_idx = attributes_place.index("silence")
        if scores_place.shape[0] == len(attributes_place):
            scores_place_sub = np.delete(scores_place, [other_idx, silence_idx], axis=0)
            attributes_place_sub = [x for x in attributes_place if (x != "other" and x != "silence")]
        else:
            print("columns number of place score matrix is different with number of place attributes.")
        concatenated_scores = np.concatenate((scores_manner, scores_place_sub), axis=0)
        concatenated_attributes = attributes["manner"] + attributes_place_sub
        df_concatenated_scores = pd.DataFrame(concatenated_scores.transpose(), columns=concatenated_attributes)
        filename = os.path.join(filepath_res, title+".xlsx")
        df_concatenated_scores.to_excel(filename)
        # to compute dynamic features.
        file = pd.read_excel(filename)
        features_raw = file.values[:, 1:]
        columns = list(file.columns[1:])
        delta1 = librosa.feature.delta(features_raw, width=dynamic_width, order=1, axis=0, mode='interp')
        delta2 = librosa.feature.delta(features_raw, width=dynamic_width, order=2, axis=0, mode='interp')
        features_raw_delta1_delta2 = np.hstack((features_raw, delta1, delta2))
        columns_delta1 = [i+'_d1' for i in columns]
        columns_delta2 = [i+'_d2' for i in columns]
        columns_new = columns + columns_delta1 + columns_delta2
        df_features_raw_delta1_delta2 = pd.DataFrame(features_raw_delta1_delta2, columns=columns_new)
        filename_new = filename[0:filename.index('.xlsx')]+'_dynamics.xlsx'
        df_features_raw_delta1_delta2.to_excel(filename_new)
        os.remove(filename)
        
    else:
        print("score matrix of manner has {} frames but score matrix of place has {} frames".format(scores_manner.shape[1], scores_place.shape[1]))
    return concatenated_scores, concatenated_attributes



## segmentation
def data_segment(features_all, segment_length, segment_shift):
    segments = []
    
    seg_num = int(np.ceil((features_all.shape[0] - segment_length)/segment_shift))
    istart = 0
    iend = istart + segment_length
    segments = np.zeros((seg_num, segment_length, features_all.shape[1]))
    ii = 0
    while iend <= features_all.shape[0] and ii < seg_num:
        segment = features_all[istart:iend, :]
#         print(segment.shape)
        segments[ii] = segment
        istart = istart + segment_shift
        iend = istart + segment_length
        ii = ii + 1
    return segments



# to generate data set for training, validation and test for leave-one-speaker-out cross-validation.
# modified on 2021-3-30. save less data.
def train_val_test(filepath_res, filename, val_num):
#     filename = 'speaker_features_label_dynamics.npz'
    data = np.load(os.path.join(filepath_res, filename), allow_pickle=True)
    data_speakers = data['speaker']
    data_features = data['features']
    data_labels = data['speech'] # for regression on PC-GITA and PDSTU
    # dictionary to store the indecies of x and y data.
    dict_test = {}
    dict_val = {}
    logo = LeaveOneGroupOut()
    print('get {} splits.'.format(logo.get_n_splits(groups=data_speakers)))
    lpgo = LeavePGroupsOut(n_groups=val_num)
    for idx_train_val, idx_test in logo.split(data_features, data_labels, data_speakers):
        idx_test = list(idx_test)
        idx_train_val = list(idx_train_val)
        speaker = np.unique(data_speakers[idx_test])[0]

        dict_test[speaker] = idx_test
        speakers_train_val = data_speakers[idx_train_val]
        labels_train_val = data_labels[idx_train_val]
        features_train_val = data_features[idx_train_val]
        iter_num = 0
        splits_num = lpgo.get_n_splits(groups=speakers_train_val)
        iter_num_random = np.random.randint(0, splits_num) # to choose two speakers for validation randomly.
        for idx_train, idx_val in lpgo.split(features_train_val, labels_train_val, speakers_train_val):
            if iter_num == iter_num_random:
                idx_val = list(idx_val)
                dict_val[speaker] = np.unique(speakers_train_val[idx_val])
                print('test {}; validation {}'.format(speaker, np.unique(speakers_train_val[idx_val])))
                break
            else:
                iter_num = iter_num + 1


    np.save(os.path.join(filepath_res, 'speakers_val.npy'), dict_val)
    np.save(os.path.join(filepath_res, 'idx_test.npy'), dict_test)



# # Run experiment.
    
# parameters setting.
from numpy.random import seed
seed(1)
tf.random.set_seed(1)
filepath = '/scratch/rfyuli/MULAN-ACCENT/'
tasks = ['PC-GITA_vowel', 'PC-GITA_read', 'PC-GITA_spon', 'PDSTU_vowel', 'PDSTU_spon']
feat_types = ['sas', 'fbank_pitch', 'egemaps']

sas_dim = 17
fbank_dim = 43
egemaps_dim = 10
dynamic_width = 9
segment_length = 100 # 1s
segment_shift = 20 # 0.2s
segment_shift_warped = 100 # for warped sas features.

val_num = 4

cnn_lossfunction = tf.keras.losses.binary_crossentropy
feature_types = ["manner", "place"]
duration = 100 # 1s
attributes_sas_dynamics = ['fricative', 'glides', 'nasal', 'other', 'silence',
       'stop', 'voiced', 'vowel', 'coronal', 'dental', 'glottal', 'high',
       'labial', 'low', 'mid', 'palatal', 'velar', 'fricative_d1', 'glides_d1',
       'nasal_d1', 'other_d1', 'silence_d1', 'stop_d1', 'voiced_d1',
       'vowel_d1', 'coronal_d1', 'dental_d1', 'glottal_d1', 'high_d1',
       'labial_d1', 'low_d1', 'mid_d1', 'palatal_d1', 'velar_d1',
       'fricative_d2', 'glides_d2', 'nasal_d2', 'other_d2', 'silence_d2',
       'stop_d2', 'voiced_d2', 'vowel_d2', 'coronal_d2', 'dental_d2',
       'glottal_d2', 'high_d2', 'labial_d2', 'low_d2', 'mid_d2', 'palatal_d2',
       'velar_d2']




# generate frame-level scores for each speaker in an excel file.
task = sys.argv[1] # 'PC-GITA_vowel', 'PC-GITA_read', 'PC-GITA_spon', 'PDSTU_vowel', 'PDSTU_spon'
feat_type = sys.argv[2] # 'sas', 'fbank_pitch', 'egemaps'
# parameters definition
if feat_type == 'fbank_pitch':
    filepath_res = os.path.join(filepath, task+'/cnn-fbank/' )
    feat_dim = fbank_dim
    dynamics = ''
    attributes = list(np.arange(0,43,1))
if feat_type == 'sas':
    filepath_res = os.path.join(filepath, task+'/res/cnn' )
    feat_dim = sas_dim*3
    attributes = {"manner": ['fricative','glides','nasal','other','silence','stop','voiced','vowel'], 
      "place": ['coronal','dental','glottal','high','labial','low','mid','other','palatal','silence','velar']}
if feat_type == 'egemaps':
    filepath_res = os.path.join(filepath, task, 'opensmile/')
    feat_dim = egemaps_dim*3
    attributes = list(np.arange(0,feat_dim,1))
saved_utterances_num = 0
if feat_type == 'sas':
    excel_files = glob(os.path.join(filepath_res, '*_manner_place_dynamics.xlsx'))
    if len(excel_files) == 0:
        print('Generate frame-level scores and dynamics for each speaker saved in an excel file')
        filepath_manner = os.path.join(filepath_res, 'manner')
        filepath_place = os.path.join(filepath_res, 'place')
        [dict_utt_dur_manner, dict_utt_scores_manner, utt_list_manner, utt_num_manner] = generate_scores_utt(filepath_manner)
        [dict_utt_dur_place, dict_utt_scores_place, utt_list_place, utt_num_place] = generate_scores_utt(filepath_place)
        print('total utterances number: {}'.format(len(utt_list_manner)))
        for utt in utt_list_manner:
            scores_mat_manner = dict_utt_scores_manner[utt].transpose()
            scores_mat_place = dict_utt_scores_place[utt].transpose()
            title = '_'.join([utt, "manner", "place"])
#                 print(title)
            if scores_mat_manner.shape[1] >= segment_length:
                [concatenated_scores, concatenated_attributes] = concatenate_scores(scores_mat_manner, scores_mat_place, filepath_res, title, dynamic_width)
                saved_utterances_num = saved_utterances_num + 1
            else:
                print('{} is discarded.'.format(utt))

if feat_type == 'fbank_pitch':
    excel_files = glob(os.path.join(filepath_res, '*_fbank_pitch.xlsx'))
    if len(excel_files) == 0:
        [dict_utt_dur_manner, dict_utt_scores_manner, utt_list_manner, utt_num_manner] = generate_scores_utt(filepath_res)
        print('total utterances number: {}'.format(len(utt_list_manner)))
        for utt in utt_list_manner:
            scores_mat_manner = dict_utt_scores_manner[utt]
            if task in utt:
                title = '_'.join([utt, "fbank_pitch"])
            else:
                title = '_'.join([utt, task, 'fbank_pitch'])
#                 print(title)
            if scores_mat_manner.shape[0] >= segment_length:
                df_concatenated_scores = pd.DataFrame(scores_mat_manner, columns=attributes)
                df_concatenated_scores.to_excel(os.path.join(filepath_res, title+".xlsx"))
                saved_utterances_num = saved_utterances_num + 1
            else:
                print('{} is discarded.'.format(utt))

if feat_type == 'egemaps':
    excel_files = glob(os.path.join(filepath_res, '*_lld_dynamics.xlsx'))
    excel_files_lld = glob(os.path.join(filepath_res, '*_lld.xlsx'))
    if len(excel_files) == 0:
        for file in excel_files_lld:
            df_lld = pd.read_excel(file)
            columns = list(df_lld.columns)[1:]
            lld = df_lld.values[:, 1:]
            if lld.shape[0] >= segment_length:
                lld_d1 = librosa.feature.delta(lld, width=dynamic_width, order=1, axis=0, mode='interp')
                lld_d2 = librosa.feature.delta(lld, width=dynamic_width, order=2, axis=0, mode='interp')
                lld_dynamics = np.hstack((lld, lld_d1, lld_d2))
                columns_d1 = [i+'_d1' for i in columns]
                columns_d2 = [i+'_d2' for i in columns]
                columns_new = columns + columns_d1 + columns_d2
                df_lld_dynamics = pd.DataFrame(lld_dynamics, columns=columns_new)
                filename_new = file[0:file.index('.xlsx')]+'_dynamics.xlsx'
                df_lld_dynamics.to_excel(filename_new)
                saved_utterances_num = saved_utterances_num + 1
    #         os.remove(filename)
print('Saved utterances number: {}'.format(saved_utterances_num))

if 'PC-GITA' in task:
    df_ratings = pd.read_excel('/home/rfyuli/audio/PC-GITA/PCGITA_metadata_50PD.xlsx')
    speakers_uni = df_ratings['NAME']
if 'PDSTU' in task:
    df_ratings = pd.read_excel('/home/rfyuli/audio/PDAUDIO/PDSTU_Expert_Rater_Data.xlsx')
    speakers_uni = df_ratings['speaker']

if feat_type == 'sas':
    filepath_res = os.path.join(filepath, task+'/res/cnn/' )
    filename_npz = os.path.join(filepath_res, 'speaker_features_label_dynamics.npz')
if feat_type == 'fbank_pitch':    
    filepath_res = os.path.join(filepath, task+'/cnn-fbank/' )
    filename_npz = os.path.join(filepath_res, 'speaker_features_label.npz')
if feat_type == 'egemaps':
    filepath_res = os.path.join(filepath, task, 'opensmile/')
    filename_npz = os.path.join(filepath_res, 'speaker_features_label_egemaps_lld_dynamics.npz')
if os.path.exists(filename_npz) == False:
    print('Data segmentation and save the speaker-features-label in a npz file')
    data_speaker = None
    data_features = None
    data_speech = None
    data_voice = None
    data_overall = None
    data_UPDRS = None
    
    for speaker in speakers_uni:
        if 'PC-GITA' in task:
            idx = df_ratings[df_ratings.NAME == speaker].index.tolist()
            label_UPDRS = df_ratings['UPDRS'][idx[0]]
            label_speech = df_ratings['UPDRS-speech'][idx[0]]
        if 'PDSTU' in task:
            idx = df_ratings[df_ratings.speaker == speaker].index.tolist()
            label_speech = df_ratings['Intelligibility_Avg.'][idx[0]]
            label_voice = df_ratings['Voice_Avg.'][idx[0]]
            label_overall = df_ratings['Overall_Avg.'][idx[0]]
        if feat_type == 'sas':
            filenames = glob(os.path.join(filepath_res, speaker+'_*'+'_manner_place_dynamics.xlsx')) # 指代不明问题
        if feat_type == 'fbank_pitch':    
            filenames = glob(os.path.join(filepath_res, speaker+'_*'+'_fbank_pitch.xlsx')) # 指代不明问题
        if feat_type == 'egemaps':
            filenames = glob(os.path.join(filepath_res, speaker+'_*_lld_dynamics.xlsx'))    
        for filename in filenames:
            print(filename)
            file = pd.read_excel(filename)
            features_all = file.values
            features_all = features_all[:, 1:]
            segments = data_segment(features_all=features_all, segment_length=segment_length, segment_shift=segment_shift)
            seg_num = len(segments)
            seg_speakers = np.repeat(speaker, seg_num)
            if 'PC-GITA' in task:
                seg_UPDRS = np.repeat(label_UPDRS, seg_num)
                seg_speech = np.repeat(label_speech, seg_num)
            if 'PDSTU' in task:                
                seg_speech = np.repeat(label_speech, seg_num)
                seg_voice = np.repeat(label_voice, seg_num)
                seg_overall = np.repeat(label_overall, seg_num)
            if data_features is None:
                data_features = segments
                data_speaker = seg_speakers
                if 'PC-GITA' in task:
                    data_UPDRS = seg_UPDRS
                    data_speech = seg_speech
                if 'PDSTU' in task:
                    data_speech = seg_speech
                    data_voice = seg_voice
                    data_overall = seg_overall
            else:        
                data_features= np.vstack((data_features, segments))
                data_speaker = np.hstack((data_speaker, seg_speakers))
                if 'PC-GITA' in task:
                    data_UPDRS = np.hstack((data_UPDRS, seg_UPDRS))
                    data_speech = np.hstack((data_speech, seg_speech))
                if 'PDSTU' in task:
                    data_speech = np.hstack((data_speech,seg_speech))
                    data_voice = np.hstack((data_voice, seg_voice))
                    data_overall = np.hstack((data_overall,seg_overall))

    if 'PC-GITA' in task:
        np.savez(filename_npz, speaker=data_speaker, features=data_features, UPDRS=data_UPDRS, speech=data_speech)
    if 'PDSTU' in task:
        np.savez(filename_npz, speaker=data_speaker, features=data_features, speech=data_speech, voice=data_voice, overall=data_overall)



if os.path.exists(os.path.join(filepath_res, 'speakers_val.npy')) == False:
    print('to generate data for training, validation and test.')
    if feat_type == 'sas':
        filepath_res = os.path.join(filepath, task+'/res/cnn/' )
        filename_npz = os.path.join(filepath_res, 'speaker_features_label_dynamics.npz')
    if feat_type == 'fbank_pitch':    
        filepath_res = os.path.join(filepath, task+'/cnn-fbank/' )
        filename_npz = os.path.join(filepath_res, 'speaker_features_label.npz')
    if feat_type == 'egemaps':
        filepath_res = os.path.join(filepath, task, 'opensmile/')
        filename_npz = os.path.join(filepath_res, 'speaker_features_label_egemaps_lld_dynamics.npz')
    print(filename_npz, val_num)
    train_val_test(filepath_res=filepath_res, filename=filename_npz, val_num=val_num)

## to generate data for MFCC from fbank_pitch data.
# filepath = '/scratch/rfyuli/MULAN-ACCENT/'
# datasets = ['PDSTU','PC-GITA']
# tasks = ['vowel', 'read', 'spon']
if feat_type == 'fbank_pitch':
    num_ceps = 12
    dynamic_width = 9
    mfcc_dim = 3*(1+num_ceps)

    filepath_ori = os.path.join(filepath_res, 'cnn-fbank')
    filepath_new = os.path.join(filepath_res, 'mfcc')
    shutil.copy(os.path.join(filepath_ori, 'idx_test.npy'), \
               os.path.join(filepath_new, 'idx_test.npy'))
    shutil.copy(os.path.join(filepath_ori, 'speakers_val.npy'), \
               os.path.join(filepath_new, 'speakers_val.npy'))

    if os.path.exists(filepath_new) == False:
        os.makedirs(filepath_new)
    os.makedirs(os.path.join(filepath_new, 'T-CONV2D_regression'))
    print(filepath_new)
    filename = 'speaker_features_label.npz'
    if os.path.exists(os.path.join(filepath_new, filename)) == False:
        data = np.load(os.path.join(filepath_ori, filename))
        files = data.files # a list, containing 'speaker', 'feautures' and other labels.
        data_speakers = data['speaker']
        data_features = data['features']
        (A, B, C) = data_features.shape
        data_mfcc = np.zeros((A, B, mfcc_dim))
        for a in range(A):
            data_fbank_cur = data_features[a, :, :40]
            data_mfcc_cur = dct(data_fbank_cur, type=2, axis=-1, norm='ortho')[:, 0:(num_ceps + 1)]
            data_mfcc_cur_d1 = librosa.feature.delta(data_mfcc_cur, width=dynamic_width, order=1, axis=0, mode='interp')
            data_mfcc_cur_d2 = librosa.feature.delta(data_mfcc_cur, width=dynamic_width, order=2, axis=0, mode='interp')
            data_mfcc[a, :, :] = np.hstack((data_mfcc_cur, data_mfcc_cur_d1, data_mfcc_cur_d2))
        if 'PDSTU' in task:
            np.savez(os.path.join(filepath_new, filename), speaker=data_speakers,\
                     features=data_mfcc, overall=data['overall'],\
                    speech=data['speech'], voice=data['voice'])
            os.makedirs(os.path.join(filepath_new, 'T-CONV2D_regression', 'overall'))
            os.makedirs(os.path.join(filepath_new, 'T-CONV2D_regression', 'speech'))
            os.makedirs(os.path.join(filepath_new, 'T-CONV2D_regression', 'voice'))
        if 'PC-GITA' in task:
            os.makedirs(os.path.join(filepath_new, 'T-CONV2D_regression', 'UPDRS'))
            os.makedirs(os.path.join(filepath_new, 'T-CONV2D_regression', 'speech'))
            np.savez(os.path.join(filepath_new, filename), speaker=data_speakers,\
                     features=data_mfcc, UPDRS=data['UPDRS'],\
                     speech=data['speech'])
    
end = time.time()
running_time = round((end-start)/60, 2)
print('- running time: {} minutes'.format(running_time))

